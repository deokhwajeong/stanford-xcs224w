{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deokhwajeong/stanford-xcs224w/blob/main/XCS224W_Colab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuXWJLEm2UWS"
      },
      "source": [
        "# **CS224W - Colab 2**\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/scpd-proed/XCS224W-Colab2/blob/main/Notebook/XCS224W_Colab2.ipynb)\n",
        "\n",
        "Before opening the colab with the badge, you would need to allow Google Colab to access the GitHub private repositories. Please check therefore [this tutorial](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb#:~:text=Navigate%20to%20http%3A%2F%2Fcolab,to%20read%20the%20private%20files.).\n",
        "\n",
        "If colab is opened with this badge, make sure please **save copy to drive** in 'File' menu before running the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gzsP50bF6Gb"
      },
      "source": [
        "In Colab 2, you will construct your first graph neural network using PyTorch Geometric (PyG) and apply the model on two Open Graph Benchmark (OGB) datasets. These two datasets will be used to benchmark your model's performance on two different graph-based tasks: 1) node property prediction (predicting the properties of single nodes) and 2) graph property prediction (predicting properties of entire graphs or subgraphs).\n",
        "\n",
        "First, you will learn how PyTorch Geometric stores graphs as PyTorch tensors.\n",
        "\n",
        "Then, you will load and inspect one of the Open Graph Benchmark (OGB) datasets by using the `ogb` package. OGB is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. The `ogb` package not only provides data loaders for each dataset but also model evaluators.\n",
        "\n",
        "Lastly, you will build our own graph neural network using PyTorch Geometric. You will train and evaluate you model on the OGB node property prediction and graph property prediction tasks.\n",
        "\n",
        "**Note**: Make sure to **sequentially run all the cells in each section**, so that the intermediate variables / packages will carry over to the next cell\n",
        "\n",
        "Have fun and good luck on Colab 2 :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEBZncyfK80M"
      },
      "source": [
        "## Building + Debugging Notes\n",
        "While working through this Colab and future Colabs, we strongly encourage you to follow a couple of building / debugging strategies:\n",
        "- During debugging make sure to run your notebook using the CPU runtime. You can change the notebook runtime by selecting `Runtime` and then `Change runtime type`. From the dropdown, select `None` as the `hardware accelerator`.\n",
        "- When working with PyTorch and Neural Network models, understanding the shapes of different tensors, especially the input and output tensors is incredibly helpful.\n",
        "- When training models, it is helpful to start by only running 1 epoch or even just a couple of batch iterations. This way you can check that all your tensor shapes and logic match up, while also tracking expected behavior, such as a decreasing training loss. Remember to comment out / save the default number of epochs that we provide you.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGKqVEbbMEzf"
      },
      "source": [
        "# Device\n",
        "For the final testing of your models you will want to use a GPU for this Colab to run quickly.\n",
        "\n",
        "Please click `Runtime` and then `Change runtime type`. Then set the `hardware accelerator` to **GPU**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCK7krJdp4o8"
      },
      "source": [
        "# Setup\n",
        "As discussed in Colab 0 and 1, the installation of PyG on Colab can be a little bit tricky. First let us check which version of PyTorch you are running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDtJPI0jqusq",
        "outputId": "2f72de0e-121c-47c4-8b5f-310e7880a1ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch\n",
            "Collecting torch==2.5.1+cu124\n",
            "  Downloading https://download.pytorch.org/whl/cu124/torch-2.5.1%2Bcu124-cp311-cp311-linux_x86_64.whl (908.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1+cu124) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1+cu124) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1+cu124) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1+cu124) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1+cu124) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1+cu124)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1+cu124)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1+cu124)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1+cu124)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1+cu124)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1+cu124)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1+cu124)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1+cu124)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1+cu124)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1+cu124) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1+cu124) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1+cu124)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch==2.5.1+cu124)\n",
            "  Downloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1+cu124) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1+cu124) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1+cu124) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.5.1+cu124 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.5.1+cu124 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch-2.5.1+cu124 triton-3.1.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Install PyTorch\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "    !pip install torch==2.5.1+cu124 -f https://download.pytorch.org/whl/torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vkP8pA1qBE5",
        "outputId": "8ae0e289-7e86-4294-ac24-f6deb5bb729f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch has version 2.5.1+cu124\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"PyTorch has version {}\".format(torch.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6d22O6DqGSZ"
      },
      "source": [
        "Download the necessary packages for PyG. Make sure that your version of torch matches the output from the cell above. In case of any issues, more information can be found on the [PyG's installation page](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr8hfxJ-qRg2",
        "outputId": "2f50e5ad-f73e-4de0-fae0-b6905fe3512f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_scatter-2.1.2%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt25cu124\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_sparse-0.6.18%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.2)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt25cu124\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.0.2)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.6.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from ogb) (2.4.0)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.11/dist-packages (from outdated>=0.2.0->ogb) (75.2.0)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from outdated>=0.2.0->ogb) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->ogb) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->ogb) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->ogb) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->ogb) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->ogb) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->outdated>=0.2.0->ogb) (2025.1.31)\n",
            "Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Downloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.4 ogb-1.3.6 outdated-0.2.2\n"
          ]
        }
      ],
      "source": [
        "# Install torch geometric\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
        "  !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-2.5.1+cu124.html\n",
        "  !pip install torch-geometric\n",
        "  !pip install ogb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QEKASRm9qusr",
        "outputId": "17267ade-b5a7-4619-cfb0-f05b38f4679e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.6.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import torch_geometric\n",
        "torch_geometric.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwwq0nSdmsOL"
      },
      "source": [
        "# 1) PyTorch Geometric (Datasets and Data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf7vUmdNKCjA"
      },
      "source": [
        "PyTorch Geometric has two classes for storing and/or transforming graphs into tensor format. One is `torch_geometric.datasets`, which contains a variety of common graph datasets. Another is `torch_geometric.data`, the class which provides the data handling of graphs as PyTorch tensors.\n",
        "\n",
        "In this section, you will learn how to use `torch_geometric.datasets` and `torch_geometric.data` together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic-o1P3r6hr2"
      },
      "source": [
        "## PyG Datasets\n",
        "\n",
        "The `torch_geometric.datasets` class has many common graph datasets. Here you will explore its usage through one example dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zT5qca3x6XpG",
        "outputId": "2d4402b2-d427-4368-ab5f-dbfb8ce5bcae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/ENZYMES.zip\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENZYMES(600)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "# Load the ENZYMES dataset (600 graphs)\n",
        "root = './enzymes'\n",
        "name = 'ENZYMES'\n",
        "pyg_dataset = TUDataset(root, name)\n",
        "print(pyg_dataset)   # should output: ENZYMES(600)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLm5vVYMAP2x"
      },
      "source": [
        "## Question 1: How many classes and features are in the ENZYMES dataset? (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iF_Kyqr_JbY",
        "outputId": "ab0f7995-9c01-40db-8c9f-88d955f10f26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENZYMES dataset has 6 classes\n",
            "ENZYMES dataset has 3 features\n"
          ]
        }
      ],
      "source": [
        "# ──────────────────────────────────────────────────────────────\n",
        "# Question 1 함수 구현 및 결과 출력\n",
        "\n",
        "def get_num_classes(pyg_dataset):\n",
        "    \"\"\"PyG dataset 객체에서 클래스 수를 반환.\"\"\"\n",
        "    return pyg_dataset.num_classes\n",
        "\n",
        "def get_num_features(pyg_dataset):\n",
        "    \"\"\"PyG dataset 객체에서 노드 특성(feature) 차원을 반환.\"\"\"\n",
        "    return pyg_dataset.num_node_features\n",
        "\n",
        "# 결과 계산\n",
        "num_classes  = get_num_classes(pyg_dataset)\n",
        "num_features = get_num_features(pyg_dataset)\n",
        "\n",
        "# 출력\n",
        "print(f\"{name} dataset has {num_classes} classes\")\n",
        "print(f\"{name} dataset has {num_features} features\")\n",
        "# ──────────────────────────────────────────────────────────────\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwKbzhHUAckZ"
      },
      "source": [
        "## PyG Data\n",
        "\n",
        "Each PyG dataset stores a list of `torch_geometric.data.Data` objects, where each `torch_geometric.data.Data` object represents a graph. You can easily get the `Data` object by indexing into the dataset.\n",
        "\n",
        "For more information such as what is stored in the `Data` object, please refer to the [documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sCV3xJWCddX"
      },
      "source": [
        "## Question 2: What is the label of the graph with index 100 in the ENZYMES dataset? (1 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIis9oTZAfs3",
        "outputId": "a62785ee-c27c-4bd7-b18b-519c5daf9814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(edge_index=[2, 168], x=[37, 3], y=[1])\n",
            "Graph with index 100 has label 4\n"
          ]
        }
      ],
      "source": [
        "# Question 2: ENZYMES dataset에서 index 100 그래프의 레이블을 구하기\n",
        "\n",
        "def get_graph_class(pyg_dataset, idx):\n",
        "    # 입력된 인덱스의 그래프 y 값을 정수로 반환\n",
        "    return int(pyg_dataset[idx].y)\n",
        "\n",
        "# 데이터셋 직접 확인 (Gradescope guard 제거)\n",
        "graph_0 = pyg_dataset[0]\n",
        "print(graph_0)\n",
        "\n",
        "# index 100 그래프 레이블 출력\n",
        "idx = 100\n",
        "label = get_graph_class(pyg_dataset, idx)\n",
        "print(f\"Graph with index {idx} has label {label}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKhcVeAhCwoY"
      },
      "source": [
        "## Question 3: How many edges does the graph with index 200 have? (1 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5m2DOfhBtWv",
        "outputId": "8c34bdf7-1100-49cf-e230-635bb4e05aee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(edge_index=[2, 106], x=[29, 3], y=[1])\n",
            "Graph with index 200 has 53 edges\n"
          ]
        }
      ],
      "source": [
        "# Question 3: ENZYMES dataset에서 index 200 그래프의 엣지 개수 구하기\n",
        "\n",
        "def get_graph_num_edges(pyg_dataset, idx):\n",
        "    \"\"\"\n",
        "    입력된 인덱스의 그래프에서 undirected edge 수를 반환.\n",
        "    edge_index 텐서가 [2, 2 * num_edges] 형태이므로 절반(/2) 해줌.\n",
        "    \"\"\"\n",
        "    edge_index = pyg_dataset[idx].edge_index\n",
        "    return edge_index.size(1) // 2\n",
        "\n",
        "# 직접 그래프 정보 확인\n",
        "graph_200 = pyg_dataset[200]\n",
        "print(graph_200)   # Data(edge_index=[2, XXX], ...)\n",
        "\n",
        "# index 200의 엣지 개수 출력\n",
        "idx = 200\n",
        "num_edges = get_graph_num_edges(pyg_dataset, idx)\n",
        "print(f\"Graph with index {idx} has {num_edges} edges\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXa7yIG4E0Fp"
      },
      "source": [
        "# 2) Open Graph Benchmark (OGB)\n",
        "\n",
        "The Open Graph Benchmark (OGB) is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. Its datasets are automatically downloaded, processed, and split using the OGB Data Loader. A model's performance over these datasets can then be evaluated using the OGB Evaluator in a unified manner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnazPGGAJAZN"
      },
      "source": [
        "## Dataset and Data\n",
        "\n",
        "OGB also supports PyG dataset and data classes. Here you will explore the `ogbn-arxiv` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gpc6bTm3GF02",
        "outputId": "04ef8b4d-d998-4117-ca22-708653f39579"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.08 GB: 100%|██████████| 81/81 [00:01<00:00, 49.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 5127.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2669.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Done!\n",
            "/usr/local/lib/python3.11/dist-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.data, self.slices = torch.load(self.processed_paths[0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ogbn-arxiv dataset has 1 graph\n",
            "Data(num_nodes=169343, edge_index=[2, 1166243], x=[169343, 128], node_year=[169343, 1], y=[169343, 1])\n"
          ]
        }
      ],
      "source": [
        "import torch_geometric.transforms as T\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "\n",
        "# Gradescope에서도 항상 실행되도록 guard 삭제\n",
        "dataset_name = 'ogbn-arxiv'\n",
        "dataset      = PygNodePropPredDataset(name=dataset_name, transform=None)\n",
        "print(f\"The {dataset_name} dataset has {len(dataset)} graph\")\n",
        "\n",
        "# 추출해 보기\n",
        "data = dataset[0]\n",
        "print(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw0xZJKZI-n3"
      },
      "source": [
        "## Question 4: How many features are in the ogbn-arxiv graph? (1 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZP844_nT2ZJl",
        "outputId": "ab39a3d7-a123-4223-d99d-0a6cf2cd3ff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The graph has 128 features\n"
          ]
        }
      ],
      "source": [
        "# Question 4: ogbn-arxiv 그래프의 feature 개수를 반환\n",
        "def graph_num_features(data):\n",
        "    # data.x 의 두 번째 차원이 feature 수\n",
        "    return data.x.size(1)\n",
        "\n",
        "# 바로 계산 및 출력\n",
        "num_features = graph_num_features(data)\n",
        "print(f\"The graph has {num_features} features\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DP_yEQZ0NVW"
      },
      "source": [
        "# 3) GNN: Node Property Prediction\n",
        "\n",
        "In this section you will build your first graph neural network using PyTorch Geometric. Then you will apply it to the task of node property prediction (node classification).\n",
        "\n",
        "Specifically, you will use GCN as the foundation for your graph neural network ([Kipf et al. (2017)](https://arxiv.org/pdf/1609.02907.pdf)). To do so, you will work with PyG's built-in `GCNConv` layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4CcOUEoInjD"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DCtgcHpGIpd",
        "outputId": "7c3b1f27-53a6-4bda-8294-91984e1f9e7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu124\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "print(torch.__version__)\n",
        "\n",
        "# The PyG built-in GCNConv\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IK9z0wQIwzQ"
      },
      "source": [
        "## Load and Preprocess the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ibJ0ieoIwQM",
        "outputId": "76775c9a-ca01-4461-9fee-4d0207ebb6bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded ogbn-arxiv with 1 graph\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.data, self.slices = torch.load(self.processed_paths[0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# ──────────────────────────────────────────────────────────────\n",
        "# Load and preprocess the ogbn-arxiv dataset\n",
        "\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "# 항상 실행되도록 guard 삭제\n",
        "dataset_name = 'ogbn-arxiv'\n",
        "dataset      = PygNodePropPredDataset(name=dataset_name, transform=T.ToSparseTensor())\n",
        "print(f\"Loaded {dataset_name} with {len(dataset)} graph\")\n",
        "\n",
        "# 그래프 추출 및 대칭화\n",
        "data = dataset[0]\n",
        "data.adj_t = data.adj_t.to_symmetric()\n",
        "\n",
        "# 디바이스 설정\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# 데이터와 인덱스들을 디바이스로 이동\n",
        "data = data.to(device)\n",
        "split_idx = dataset.get_idx_split()\n",
        "train_idx = split_idx['train'].to(device)\n",
        "valid_idx = split_idx['valid'].to(device)\n",
        "test_idx  = split_idx['test'].to(device)\n",
        "# ──────────────────────────────────────────────────────────────\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgUA815bNJ8w"
      },
      "source": [
        "## GCN Model\n",
        "\n",
        "Now that you have loaded the datasets, you will implement your own GCN model!\n",
        "\n",
        "Please follow the figure below to help in implementing the `forward` function.\n",
        "\n",
        "\n",
        "![test](https://drive.google.com/uc?id=128AuYAXNXGg7PIhJJ7e420DoPWKb-RtL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgspXTYpNJLA"
      },
      "outputs": [],
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
        "                 dropout, return_embeds=False):\n",
        "        super().__init__()\n",
        "        # convolution + batchnorm lists\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.bns   = torch.nn.ModuleList()\n",
        "\n",
        "        # 첫 번째 레이어 (in → hidden)\n",
        "        self.convs.append(GCNConv(input_dim, hidden_dim))\n",
        "        self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
        "\n",
        "        # 중간 레이어들 (hidden → hidden)\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
        "\n",
        "        # 마지막 레이어 (hidden → out)\n",
        "        self.convs.append(GCNConv(hidden_dim, output_dim))\n",
        "\n",
        "        # 출력용 log-softmax\n",
        "        self.soft_max = torch.nn.LogSoftmax(dim=1)\n",
        "        self.dropout  = dropout\n",
        "        self.return_embeds = return_embeds\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "        # 중간 레이어들: conv → BN → ReLU → Dropout\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, adj_t)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        # 마지막 레이어\n",
        "        x = self.convs[-1](x, adj_t)\n",
        "        if not self.return_embeds:\n",
        "            x = self.soft_max(x)\n",
        "        return x\n",
        "# 잘 정의되었는지 테스트\n",
        "# model = GCN(input_dim=128, hidden_dim=64, output_dim=5, num_layers=3, dropout=0.5)\n",
        "# model.eval()  # 평가 모드로 전환\n",
        "# # 노드(feature) 10개, 피쳐 차원 128짜리 더미\n",
        "# x_dummy = torch.randn(10, 128)\n",
        "# # 임의의 adjacency in sparse tensor 형식 (여기서는 완전 연결 그래프 예시)\n",
        "# adj_dummy = torch.eye(10).to_sparse()\n",
        "# out = model(x_dummy, adj_dummy)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF1hnHUhO81e"
      },
      "outputs": [],
      "source": [
        "def train(model, data, train_idx, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 모델 출력\n",
        "    out = model(data.x, data.adj_t)\n",
        "\n",
        "    # 학습 손실 계산: target을 1-D 텐서로 변환\n",
        "    y = data.y[train_idx].view(-1)         # (N,) 형태\n",
        "    loss = loss_fn(out[train_idx], y)      # out[train_idx]: (N, num_classes)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJdlrJQhPBsK"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, evaluator, save_model_results=False):\n",
        "    model.eval()\n",
        "    out = model(data.x, data.adj_t)        # (num_nodes, num_classes)\n",
        "\n",
        "    # 예측: 가장 큰 차원 인덱스를 2-D (N,1) 형태로.\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)  # (num_nodes, 1)\n",
        "\n",
        "    # 원래 data.y 는 (num_nodes,1) 이므로, 그대로.\n",
        "    results = {}\n",
        "    for split in ['train','valid','test']:\n",
        "        mask = split_idx[split]\n",
        "        results[split] = evaluator.eval({\n",
        "            'y_true': data.y[mask],           # (Ns,1)\n",
        "            'y_pred': y_pred[mask],           # (Ns,1)\n",
        "        })['acc']\n",
        "\n",
        "    # 예측 저장\n",
        "    if save_model_results:\n",
        "        import pandas as pd\n",
        "        df = pd.DataFrame({'y_pred': y_pred.cpu().numpy().squeeze()})\n",
        "        df.to_csv('ogbn-arxiv_node.csv', index=False)\n",
        "\n",
        "    return results['train'], results['valid'], results['test']\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7F46xkuLiOL"
      },
      "outputs": [],
      "source": [
        "# Please do not change the args\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  args = {\n",
        "      'device': device,\n",
        "      'num_layers': 3,\n",
        "      'hidden_dim': 256,\n",
        "      'dropout': 0.5,\n",
        "      'lr': 0.01,\n",
        "      'epochs': 100,\n",
        "  }\n",
        "  args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT8RyM2cPGxM"
      },
      "outputs": [],
      "source": [
        "# Gradescope 환경에서만 실행되지 않도록 guard 포함\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "    # GCN 모델 생성\n",
        "    model = GCN(\n",
        "        data.num_features,\n",
        "        args['hidden_dim'],\n",
        "        dataset.num_classes,\n",
        "        args['num_layers'],\n",
        "        args['dropout']\n",
        "    ).to(device)\n",
        "\n",
        "    # (PyG 2.3.1 / Torch 2.0.1 에서 compile 지원이 불안정하여 주석 처리)\n",
        "    # try:\n",
        "    #     model = torch_geometric.compile(model)\n",
        "    #     print(\"GCN Model compiled\")\n",
        "    # except Exception as err:\n",
        "    #     print(f\"Model compile not supported: {err}\")\n",
        "\n",
        "    # OGB evaluator 초기화\n",
        "    evaluator = Evaluator(name='ogbn-arxiv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd5O5cnPPdVF",
        "outputId": "7ad869a2-0eec-459f-e971-4ac11fbbcccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 4.2698, Train: 24.14%, Valid: 28.07% Test: 25.24%\n",
            "Epoch: 02, Loss: 2.3786, Train: 24.45%, Valid: 22.29% Test: 27.86%\n",
            "Epoch: 03, Loss: 1.9599, Train: 28.01%, Valid: 24.46% Test: 29.41%\n",
            "Epoch: 04, Loss: 1.7893, Train: 46.04%, Valid: 47.25% Test: 49.48%\n",
            "Epoch: 05, Loss: 1.6790, Train: 46.84%, Valid: 44.82% Test: 41.68%\n",
            "Epoch: 06, Loss: 1.5856, Train: 44.17%, Valid: 43.63% Test: 41.44%\n",
            "Epoch: 07, Loss: 1.5194, Train: 40.99%, Valid: 43.38% Test: 45.37%\n",
            "Epoch: 08, Loss: 1.4712, Train: 39.45%, Valid: 41.96% Test: 45.63%\n",
            "Epoch: 09, Loss: 1.4160, Train: 39.31%, Valid: 41.40% Test: 45.45%\n",
            "Epoch: 10, Loss: 1.3899, Train: 38.51%, Valid: 40.22% Test: 44.61%\n",
            "Epoch: 11, Loss: 1.3631, Train: 37.89%, Valid: 39.34% Test: 44.04%\n",
            "Epoch: 12, Loss: 1.3309, Train: 38.05%, Valid: 39.15% Test: 43.87%\n",
            "Epoch: 13, Loss: 1.3006, Train: 38.64%, Valid: 39.17% Test: 43.79%\n",
            "Epoch: 14, Loss: 1.2851, Train: 39.85%, Valid: 40.66% Test: 44.98%\n",
            "Epoch: 15, Loss: 1.2647, Train: 42.94%, Valid: 44.86% Test: 48.08%\n",
            "Epoch: 16, Loss: 1.2443, Train: 46.81%, Valid: 48.59% Test: 50.51%\n",
            "Epoch: 17, Loss: 1.2299, Train: 50.43%, Valid: 51.88% Test: 53.25%\n",
            "Epoch: 18, Loss: 1.2194, Train: 53.17%, Valid: 54.74% Test: 56.39%\n",
            "Epoch: 19, Loss: 1.2027, Train: 55.06%, Valid: 56.64% Test: 58.50%\n",
            "Epoch: 20, Loss: 1.1900, Train: 56.53%, Valid: 57.84% Test: 59.80%\n",
            "Epoch: 21, Loss: 1.1808, Train: 58.15%, Valid: 59.17% Test: 60.72%\n",
            "Epoch: 22, Loss: 1.1687, Train: 59.58%, Valid: 60.18% Test: 61.40%\n",
            "Epoch: 23, Loss: 1.1564, Train: 60.38%, Valid: 60.58% Test: 61.51%\n",
            "Epoch: 24, Loss: 1.1487, Train: 60.64%, Valid: 60.61% Test: 61.63%\n",
            "Epoch: 25, Loss: 1.1421, Train: 61.05%, Valid: 61.04% Test: 62.33%\n",
            "Epoch: 26, Loss: 1.1349, Train: 61.79%, Valid: 61.99% Test: 63.20%\n",
            "Epoch: 27, Loss: 1.1267, Train: 63.04%, Valid: 63.49% Test: 64.26%\n",
            "Epoch: 28, Loss: 1.1190, Train: 64.52%, Valid: 64.99% Test: 65.10%\n",
            "Epoch: 29, Loss: 1.1055, Train: 65.76%, Valid: 66.26% Test: 65.90%\n",
            "Epoch: 30, Loss: 1.1016, Train: 66.75%, Valid: 67.18% Test: 66.71%\n",
            "Epoch: 31, Loss: 1.0962, Train: 67.44%, Valid: 67.95% Test: 67.53%\n",
            "Epoch: 32, Loss: 1.0895, Train: 68.01%, Valid: 68.49% Test: 67.97%\n",
            "Epoch: 33, Loss: 1.0836, Train: 68.38%, Valid: 68.64% Test: 67.99%\n",
            "Epoch: 34, Loss: 1.0773, Train: 68.57%, Valid: 68.49% Test: 68.03%\n",
            "Epoch: 35, Loss: 1.0718, Train: 68.66%, Valid: 68.27% Test: 68.05%\n",
            "Epoch: 36, Loss: 1.0672, Train: 68.70%, Valid: 68.17% Test: 68.24%\n",
            "Epoch: 37, Loss: 1.0622, Train: 68.83%, Valid: 68.17% Test: 68.36%\n",
            "Epoch: 38, Loss: 1.0588, Train: 69.08%, Valid: 68.52% Test: 68.80%\n",
            "Epoch: 39, Loss: 1.0528, Train: 69.46%, Valid: 69.23% Test: 69.27%\n",
            "Epoch: 40, Loss: 1.0488, Train: 69.80%, Valid: 69.82% Test: 69.46%\n",
            "Epoch: 41, Loss: 1.0446, Train: 69.91%, Valid: 69.88% Test: 69.53%\n",
            "Epoch: 42, Loss: 1.0398, Train: 70.14%, Valid: 70.01% Test: 69.65%\n",
            "Epoch: 43, Loss: 1.0388, Train: 70.36%, Valid: 70.16% Test: 69.86%\n",
            "Epoch: 44, Loss: 1.0348, Train: 70.67%, Valid: 70.42% Test: 69.94%\n",
            "Epoch: 45, Loss: 1.0284, Train: 70.84%, Valid: 70.45% Test: 69.80%\n",
            "Epoch: 46, Loss: 1.0275, Train: 71.00%, Valid: 70.52% Test: 69.89%\n",
            "Epoch: 47, Loss: 1.0233, Train: 71.20%, Valid: 70.70% Test: 70.15%\n",
            "Epoch: 48, Loss: 1.0202, Train: 71.23%, Valid: 70.76% Test: 70.52%\n",
            "Epoch: 49, Loss: 1.0159, Train: 71.12%, Valid: 70.54% Test: 70.54%\n",
            "Epoch: 50, Loss: 1.0103, Train: 71.16%, Valid: 70.58% Test: 70.43%\n",
            "Epoch: 51, Loss: 1.0102, Train: 71.35%, Valid: 70.72% Test: 70.38%\n",
            "Epoch: 52, Loss: 1.0065, Train: 71.49%, Valid: 70.82% Test: 70.22%\n",
            "Epoch: 53, Loss: 1.0021, Train: 71.62%, Valid: 70.80% Test: 69.90%\n",
            "Epoch: 54, Loss: 1.0017, Train: 71.67%, Valid: 70.77% Test: 69.92%\n",
            "Epoch: 55, Loss: 0.9977, Train: 71.74%, Valid: 70.89% Test: 70.04%\n",
            "Epoch: 56, Loss: 0.9966, Train: 71.87%, Valid: 71.00% Test: 70.17%\n",
            "Epoch: 57, Loss: 0.9924, Train: 71.96%, Valid: 71.07% Test: 70.37%\n",
            "Epoch: 58, Loss: 0.9895, Train: 72.02%, Valid: 71.23% Test: 70.43%\n",
            "Epoch: 59, Loss: 0.9874, Train: 72.02%, Valid: 71.16% Test: 70.48%\n",
            "Epoch: 60, Loss: 0.9842, Train: 71.92%, Valid: 71.10% Test: 70.58%\n",
            "Epoch: 61, Loss: 0.9819, Train: 71.95%, Valid: 71.10% Test: 70.68%\n",
            "Epoch: 62, Loss: 0.9815, Train: 72.09%, Valid: 71.26% Test: 70.54%\n",
            "Epoch: 63, Loss: 0.9799, Train: 72.26%, Valid: 71.24% Test: 70.18%\n",
            "Epoch: 64, Loss: 0.9748, Train: 72.32%, Valid: 70.98% Test: 69.67%\n",
            "Epoch: 65, Loss: 0.9735, Train: 72.45%, Valid: 71.15% Test: 70.05%\n",
            "Epoch: 66, Loss: 0.9726, Train: 72.51%, Valid: 71.28% Test: 70.51%\n",
            "Epoch: 67, Loss: 0.9691, Train: 72.51%, Valid: 71.37% Test: 70.28%\n",
            "Epoch: 68, Loss: 0.9675, Train: 72.48%, Valid: 71.25% Test: 70.05%\n",
            "Epoch: 69, Loss: 0.9671, Train: 72.49%, Valid: 71.28% Test: 70.09%\n",
            "Epoch: 70, Loss: 0.9681, Train: 72.58%, Valid: 71.27% Test: 70.41%\n",
            "Epoch: 71, Loss: 0.9618, Train: 72.69%, Valid: 71.33% Test: 70.56%\n",
            "Epoch: 72, Loss: 0.9599, Train: 72.68%, Valid: 71.25% Test: 70.32%\n",
            "Epoch: 73, Loss: 0.9580, Train: 72.74%, Valid: 71.15% Test: 70.13%\n",
            "Epoch: 74, Loss: 0.9559, Train: 72.83%, Valid: 71.24% Test: 70.52%\n",
            "Epoch: 75, Loss: 0.9510, Train: 72.78%, Valid: 71.55% Test: 71.07%\n",
            "Epoch: 76, Loss: 0.9512, Train: 72.80%, Valid: 71.62% Test: 71.33%\n",
            "Epoch: 77, Loss: 0.9494, Train: 72.98%, Valid: 71.69% Test: 71.38%\n",
            "Epoch: 78, Loss: 0.9479, Train: 73.09%, Valid: 71.75% Test: 71.05%\n",
            "Epoch: 79, Loss: 0.9464, Train: 73.11%, Valid: 71.61% Test: 70.68%\n",
            "Epoch: 80, Loss: 0.9404, Train: 73.11%, Valid: 71.49% Test: 70.46%\n",
            "Epoch: 81, Loss: 0.9427, Train: 73.14%, Valid: 71.58% Test: 70.83%\n",
            "Epoch: 82, Loss: 0.9373, Train: 73.15%, Valid: 71.81% Test: 71.27%\n",
            "Epoch: 83, Loss: 0.9401, Train: 73.25%, Valid: 71.90% Test: 71.34%\n",
            "Epoch: 84, Loss: 0.9381, Train: 73.42%, Valid: 71.83% Test: 70.96%\n",
            "Epoch: 85, Loss: 0.9353, Train: 73.46%, Valid: 71.95% Test: 71.15%\n",
            "Epoch: 86, Loss: 0.9340, Train: 73.41%, Valid: 71.88% Test: 71.33%\n",
            "Epoch: 87, Loss: 0.9339, Train: 73.32%, Valid: 71.73% Test: 71.19%\n",
            "Epoch: 88, Loss: 0.9288, Train: 73.24%, Valid: 71.60% Test: 71.01%\n",
            "Epoch: 89, Loss: 0.9284, Train: 73.34%, Valid: 71.77% Test: 70.85%\n",
            "Epoch: 90, Loss: 0.9276, Train: 73.48%, Valid: 71.84% Test: 71.02%\n",
            "Epoch: 91, Loss: 0.9247, Train: 73.68%, Valid: 71.88% Test: 71.18%\n",
            "Epoch: 92, Loss: 0.9244, Train: 73.62%, Valid: 71.83% Test: 70.94%\n",
            "Epoch: 93, Loss: 0.9239, Train: 73.70%, Valid: 71.62% Test: 70.11%\n",
            "Epoch: 94, Loss: 0.9192, Train: 73.79%, Valid: 71.68% Test: 70.08%\n",
            "Epoch: 95, Loss: 0.9220, Train: 73.80%, Valid: 71.99% Test: 71.08%\n",
            "Epoch: 96, Loss: 0.9201, Train: 73.71%, Valid: 71.96% Test: 71.23%\n",
            "Epoch: 97, Loss: 0.9173, Train: 73.80%, Valid: 72.01% Test: 71.19%\n",
            "Epoch: 98, Loss: 0.9143, Train: 73.81%, Valid: 71.84% Test: 71.00%\n",
            "Epoch: 99, Loss: 0.9136, Train: 73.67%, Valid: 71.68% Test: 71.35%\n",
            "Epoch: 100, Loss: 0.9130, Train: 73.69%, Valid: 71.88% Test: 71.70%\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  # reset the parameters to initial random value\n",
        "  model.reset_parameters()\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "  loss_fn = F.nll_loss\n",
        "\n",
        "  best_model = None\n",
        "  best_valid_acc = 0\n",
        "\n",
        "  for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "    loss = train(model, data, train_idx, optimizer, loss_fn)\n",
        "    result = test(model, data, split_idx, evaluator)\n",
        "    train_acc, valid_acc, test_acc = result\n",
        "    if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        best_model = copy.deepcopy(model)\n",
        "    print(f'Epoch: {epoch:02d}, '\n",
        "          f'Loss: {loss:.4f}, '\n",
        "          f'Train: {100 * train_acc:.2f}%, '\n",
        "          f'Valid: {100 * valid_acc:.2f}% '\n",
        "          f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQtt-EKA8P4r"
      },
      "source": [
        "## Question 5: What are your `best_model` validation and test accuracies?(20 points)\n",
        "\n",
        "Run the cell below to see the results of your best model and save your model's predictions to a file named *ogbn-arxiv_node.csv*.\n",
        "\n",
        "You can view this file by clicking on the *Folder* icon on the left side pannel. As in Colab 1, when you sumbit your assignment, you will have to download this file and attatch it to your submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqcextqOL2FX",
        "outputId": "bc3dac32-91ad-4454-bb78-122f08e9bc51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model: Train: 73.80%, Valid: 72.01% Test: 71.19%\n"
          ]
        }
      ],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  best_result = test(best_model, data, split_idx, evaluator, save_model_results=True)\n",
        "  train_acc, valid_acc, test_acc = best_result\n",
        "  print(f'Best model: '\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * valid_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8pOD6y80TyI"
      },
      "source": [
        "# 4) GNN: Graph Property Prediction\n",
        "\n",
        "In this section you will create a graph neural network for graph property prediction (graph classification).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRg5VOEdQTa4"
      },
      "source": [
        "## Load and preprocess the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXb-O5QUIgTH",
        "outputId": "63a49b78-79ba-454a-dfd5-8a2cdeea306f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/graphproppred/csv_mol_download/hiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.00 GB: 100%|██████████| 3/3 [00:00<00:00,  7.54it/s]\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/hiv.zip\n",
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 41127/41127 [00:00<00:00, 69702.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 41127/41127 [00:01<00:00, 20773.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n",
            "ogbg-molhiv dataset has 41127 graphs\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n",
            "/usr/local/lib/python3.11/dist-packages/ogb/graphproppred/dataset_pyg.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.data, self.slices = torch.load(self.processed_paths[0])\n"
          ]
        }
      ],
      "source": [
        "# 4) GNN: Graph Property Prediction\n",
        "# Load and preprocess the OGBG‐MolHIV dataset for graph classification\n",
        "\n",
        "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
        "from torch_geometric.loader import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# 데이터셋 로드 (인접 행렬 변환 없이 일반 edge_index로 읽어오기)\n",
        "dataset = PygGraphPropPredDataset(name='ogbg-molhiv', transform=None)\n",
        "print(f\"{dataset.name} dataset has {len(dataset)} graphs\")\n",
        "\n",
        "# 장치 설정\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 학습/검증/테스트 인덱스 분할 정보 가져오기\n",
        "split_idx = dataset.get_idx_split()\n",
        "train_idx = split_idx['train']\n",
        "valid_idx = split_idx['valid']\n",
        "test_idx  = split_idx['test']\n",
        "\n",
        "# PyG DataLoader 생성\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(dataset[train_idx], batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(dataset[valid_idx], batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(dataset[test_idx], batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# OGB evaluator 초기화\n",
        "evaluator = Evaluator(name='ogbg-molhiv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cHHbgW1c5hi"
      },
      "outputs": [],
      "source": [
        "# Load the dataset splits into corresponding dataloaders\n",
        "# We will train the graph classification task on a batch of 32 graphs\n",
        "# Shuffle the order of graphs for training set\n",
        "\n",
        "train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=32, shuffle=True, num_workers=0)\n",
        "valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=32, shuffle=False, num_workers=0)\n",
        "test_loader  = DataLoader(dataset[split_idx[\"test\"]],  batch_size=32, shuffle=False, num_workers=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbcBPojdPfc-"
      },
      "source": [
        "## Initialize Model Training Parameters\n",
        "During debugging and testing we recommend setting `epochs` to a lower value such as 1 or 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYrSnOj0Y4DK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0961153a-3f91-4760-fc5e-ff76c7dc16a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'device': 'cuda',\n",
              " 'num_layers': 5,\n",
              " 'hidden_dim': 256,\n",
              " 'dropout': 0.5,\n",
              " 'lr': 0.001,\n",
              " 'epochs': 15,\n",
              " 'batch_size': 32}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Initialize model training parameters for OGBG-MolHIV graph classification\n",
        "import os\n",
        "\n",
        "# (Gradescope guard 제거)\n",
        "\n",
        "args = {\n",
        "    'device'    : device,   # cuda or cpu\n",
        "    'num_layers': 5,        # number of graph conv layers\n",
        "    'hidden_dim': 256,      # hidden dimensionality\n",
        "    'dropout'   : 0.5,      # dropout probability\n",
        "    'lr'        : 0.001,    # learning rate\n",
        "    'epochs'    : 15,       # number of training epochs\n",
        "    'batch_size': 32        # batch size for DataLoader\n",
        "}\n",
        "\n",
        "args\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WLhguSTeazy"
      },
      "source": [
        "## Graph Prediction Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u05Z14TRYPGn"
      },
      "source": [
        "### Graph Mini-Batching\n",
        "Before diving into the actual model, we introduce the concept of mini-batching with graphs. In order to parallelize the processing of a mini-batch of graphs, PyG combines the graphs into a single disconnected graph data object (*torch_geometric.data.Batch*). *torch_geometric.data.Batch* inherits from *torch_geometric.data.Data* (introduced earlier) and contains an additional attribute called `batch`.\n",
        "\n",
        "The `batch` attribute is a vector mapping each node to the index of its corresponding graph within the mini-batch:\n",
        "\n",
        "    batch = [0, ..., 0, 1, ..., n - 2, n - 1, ..., n - 1]\n",
        "\n",
        "This attribute is crucial for associating which graph each node belongs to and can be used to e.g. average the node embeddings for each graph individually to compute graph level embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pcic9NNU3nGK"
      },
      "source": [
        "### Implementation\n",
        "Now, you have all of the tools to implement a GCN Graph Prediction model!  \n",
        "\n",
        "To do so, you will reuse the your existing GCN model to generate `node_embeddings` for a graph and then use `Global Pooling` over these node embeddings to create a graph level embeddings that can be used to predict graph properties. Remeber that the `batch` attribute will be essential for performining Global Pooling over our mini-batch of graphs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrrJ2TgNUIhf"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_Kq3zyjeZ22"
      },
      "outputs": [],
      "source": [
        "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "class GCN_Graph(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim, output_dim, num_layers, dropout):\n",
        "        super(GCN_Graph, self).__init__()\n",
        "\n",
        "        # 1) Atom feature encoder\n",
        "        self.node_encoder = AtomEncoder(hidden_dim)\n",
        "\n",
        "        # 2) Reuse your prior node-level GCN (return_embeds=True 로 설정되어 있어야 함)\n",
        "        self.gnn_node = GCN(\n",
        "            hidden_dim,   # input_dim = hidden_dim (encoded atom features)\n",
        "            hidden_dim,   # hidden_dim of GCN\n",
        "            hidden_dim,   # output_dim = hidden_dim (we only need embeddings)\n",
        "            num_layers,\n",
        "            dropout,\n",
        "            return_embeds=True\n",
        "        )\n",
        "\n",
        "        # 3) Global pooling layer\n",
        "        self.pool = global_mean_pool\n",
        "\n",
        "        # 4) 최종 그래프 레벨 예측을 위한 Linear\n",
        "        self.linear = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # AtomEncoder에는 reset_parameters가 없으므로 호출하지 않음.\n",
        "        # self.node_encoder.reset_parameters()\n",
        "\n",
        "        # 실제 초기화가 필요한 부분만 호출\n",
        "        self.gnn_node.reset_parameters()\n",
        "        self.linear.reset_parameters()\n",
        "\n",
        "    def forward(self, batched_data):\n",
        "        # batched_data: torch_geometric.data.Batch\n",
        "        x      = batched_data.x\n",
        "        edge_index = batched_data.edge_index\n",
        "        batch  = batched_data.batch   # 그래프별 노드 소속 인덱스\n",
        "\n",
        "        # 1) AtomEncoder → GCN 으로 노드 임베딩 생성\n",
        "        x = self.node_encoder(x)\n",
        "        x = self.gnn_node(x, edge_index)\n",
        "\n",
        "        # 2) 노드 임베딩을 그래프 단위 임베딩으로 집계\n",
        "        x = self.pool(x, batch)\n",
        "\n",
        "        # 3) 그래프 레벨 속성 예측\n",
        "        out = self.linear(x)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJjnGuMSbjX0"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, device, data_loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for step, batch in enumerate(tqdm(data_loader, desc=\"Iteration\")):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        # 1) gradient 초기화\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 2) 모델에 통과시켜 예측값 얻기\n",
        "        out = model(batch)  # size = [batch_size] 혹은 [batch_size, 1]\n",
        "\n",
        "        # 3) 레이블된 샘플만 선택\n",
        "        #    OGB-MolHIV 예시: y==-1 이면 레이블 없음\n",
        "        mask = batch.y != -1\n",
        "        if mask.sum() == 0:\n",
        "            continue  # 레이블된 그래프가 하나도 없으면 건너뛰기\n",
        "\n",
        "        # 4) loss 계산\n",
        "        #    batch.y 를 float32 로 바꿔서 MSE나 BCE 등에 사용\n",
        "        y_true = batch.y[mask].to(torch.float32)\n",
        "        y_pred = out[mask].view(-1)\n",
        "\n",
        "        loss = loss_fn(y_pred, y_true)\n",
        "\n",
        "        # 5) backprop + step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / (step + 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztPHXq_Gzn7U"
      },
      "outputs": [],
      "source": [
        "# The evaluation function\n",
        "def eval(model, device, loader, evaluator, save_model_results=False, save_file=None):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        if batch.x.shape[0] == 1:\n",
        "            pass\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                pred = model(batch)\n",
        "\n",
        "            y_true.append(batch.y.view(pred.shape).detach().cpu())\n",
        "            y_pred.append(pred.detach().cpu())\n",
        "\n",
        "    y_true = torch.cat(y_true, dim = 0).numpy()\n",
        "    y_pred = torch.cat(y_pred, dim = 0).numpy()\n",
        "\n",
        "    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
        "\n",
        "    if save_model_results:\n",
        "        print (\"Saving Model Predictions\")\n",
        "\n",
        "        # Create a pandas dataframe with a two columns\n",
        "        # y_pred | y_true\n",
        "        data = {}\n",
        "        data['y_pred'] = y_pred.reshape(-1)\n",
        "        data['y_true'] = y_true.reshape(-1)\n",
        "\n",
        "        df = pd.DataFrame(data=data)\n",
        "        # Save to csv\n",
        "        df.to_csv('ogbg-molhiv_graph_' + save_file + '.csv', sep=',', index=False)\n",
        "\n",
        "    return evaluator.eval(input_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MR1wQ4hMZeMw"
      },
      "outputs": [],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  model = GCN_Graph(args['hidden_dim'],\n",
        "              dataset.num_tasks, args['num_layers'],\n",
        "              args['dropout']).to(device)\n",
        "  # Disable compile as this does not seem to work yet in PyTorch 2.0.1/PyG 2.3.1\n",
        "  # try:\n",
        "  #   model = torch_geometric.compile(model)\n",
        "  #   print(\"Graph Prediction Model compiled\")\n",
        "  # except Exception as err:\n",
        "  #   print(f\"Model compile not supported: {err}\")\n",
        "\n",
        "  evaluator = Evaluator(name='ogbg-molhiv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJGTNZiuZy0A",
        "outputId": "f4bb73e2-9c41-42e4-d9dc-fa5b547a8ba9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:16<00:00, 62.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:07<00:00, 134.66it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:01<00:00, 104.04it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:01<00:00, 105.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 0.1568, Train: 72.18%, Valid: 74.10% Test: 69.49%\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:13<00:00, 77.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:07<00:00, 134.71it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 147.35it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 146.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 02, Loss: 0.1482, Train: 75.97%, Valid: 76.18% Test: 73.22%\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:16<00:00, 62.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:07<00:00, 134.38it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 152.21it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 151.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 03, Loss: 0.1454, Train: 76.18%, Valid: 73.96% Test: 65.14%\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:12<00:00, 84.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:07<00:00, 142.23it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:01<00:00, 107.80it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 142.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 04, Loss: 0.1426, Train: 77.22%, Valid: 74.59% Test: 72.50%\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:12<00:00, 84.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:07<00:00, 146.06it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 146.06it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 146.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 05, Loss: 0.1409, Train: 77.39%, Valid: 74.58% Test: 68.36%\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:12<00:00, 83.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:07<00:00, 134.63it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 145.75it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 150.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 06, Loss: 0.1396, Train: 78.48%, Valid: 74.93% Test: 69.81%\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:12<00:00, 79.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:07<00:00, 134.68it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 146.86it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 143.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 07, Loss: 0.1385, Train: 78.61%, Valid: 75.22% Test: 73.51%\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:12<00:00, 83.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:07<00:00, 133.94it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 144.52it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 143.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 08, Loss: 0.1377, Train: 78.84%, Valid: 74.40% Test: 70.95%\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:12<00:00, 83.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:07<00:00, 141.73it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:01<00:00, 106.01it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 145.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 09, Loss: 0.1360, Train: 80.21%, Valid: 75.34% Test: 74.15%\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:12<00:00, 84.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:07<00:00, 145.49it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 148.18it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 151.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10, Loss: 0.1351, Train: 79.77%, Valid: 77.35% Test: 73.30%\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:12<00:00, 84.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:07<00:00, 135.08it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 147.23it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 146.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 11, Loss: 0.1340, Train: 79.42%, Valid: 73.36% Test: 70.74%\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:12<00:00, 84.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:07<00:00, 136.01it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 148.75it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 145.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 12, Loss: 0.1325, Train: 78.22%, Valid: 75.62% Test: 74.18%\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:12<00:00, 85.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:07<00:00, 134.47it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 148.74it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 145.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 13, Loss: 0.1330, Train: 80.85%, Valid: 75.74% Test: 73.01%\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:12<00:00, 84.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:07<00:00, 146.15it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:01<00:00, 127.15it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:01<00:00, 112.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 14, Loss: 0.1314, Train: 80.43%, Valid: 75.94% Test: 71.00%\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:12<00:00, 85.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:07<00:00, 142.77it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 148.65it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 147.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 15, Loss: 0.1315, Train: 81.78%, Valid: 75.81% Test: 72.01%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  model.reset_parameters()\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "  loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "  best_model = None\n",
        "  best_valid_acc = 0\n",
        "\n",
        "  for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "    print('Training...')\n",
        "    loss = train(model, device, train_loader, optimizer, loss_fn)\n",
        "\n",
        "    print('Evaluating...')\n",
        "    train_result = eval(model, device, train_loader, evaluator)\n",
        "    val_result = eval(model, device, valid_loader, evaluator)\n",
        "    test_result = eval(model, device, test_loader, evaluator)\n",
        "\n",
        "    train_acc, valid_acc, test_acc = train_result[dataset.eval_metric], val_result[dataset.eval_metric], test_result[dataset.eval_metric]\n",
        "    if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        best_model = copy.deepcopy(model)\n",
        "    print(f'Epoch: {epoch:02d}, '\n",
        "          f'Loss: {loss:.4f}, '\n",
        "          f'Train: {100 * train_acc:.2f}%, '\n",
        "          f'Valid: {100 * valid_acc:.2f}% '\n",
        "          f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I17-Qso_n88"
      },
      "source": [
        "## Question 6: What are your `best_model` validation and test ROC-AUC scores? (20 points)\n",
        "\n",
        "Run the cell below to see the results of your best model and save your model's predictions over the validation and test datasets. The resulting files are named *ogbg-molhiv_graph_valid.csv* and *ogbg-molhiv_graph_test.csv*.\n",
        "\n",
        "Again, you can view these files by clicking on the *Folder* icon on the left side pannel. As in Colab 1, when you sumbit your assignment, you will have to download these files and attatch them to your submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oq5QaG21dOOO",
        "outputId": "5e1421b8-3893-4faa-fb9f-fc34b1200c1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 1029/1029 [00:07<00:00, 134.99it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 144.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Model Predictions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Iteration: 100%|██████████| 129/129 [00:00<00:00, 145.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Model Predictions\n",
            "Best model: Train: 79.77%, Valid: 77.35% Test: 73.30%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  train_acc = eval(best_model, device, train_loader, evaluator)[dataset.eval_metric]\n",
        "  valid_acc = eval(best_model, device, valid_loader, evaluator, save_model_results=True, save_file=\"valid\")[dataset.eval_metric]\n",
        "  test_acc  = eval(best_model, device, test_loader, evaluator, save_model_results=True, save_file=\"test\")[dataset.eval_metric]\n",
        "\n",
        "  print(f'Best model: '\n",
        "      f'Train: {100 * train_acc:.2f}%, '\n",
        "      f'Valid: {100 * valid_acc:.2f}% '\n",
        "      f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBi_t8n0iZ4P"
      },
      "source": [
        "## Question 7 (Optional): Experiment with the two other global pooling layers in Pytorch Geometric."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7: 글로벌 pooling 비교 (data.x.float() 추가 버전)\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool, global_add_pool, global_max_pool\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# 1) GNNConvLayer 정의\n",
        "class GNNConvLayer(torch.nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(GCNConv(in_dim, hidden_dim))\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
        "    def forward(self, x, edge_index):\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index).relu()\n",
        "        return x\n",
        "\n",
        "# 2) GraphPoolNet 정의 (x.float() 추가)\n",
        "class GraphPoolNet(torch.nn.Module):\n",
        "    def __init__(self, conv_layers, hidden_dim, num_classes, pool_fn):\n",
        "        super().__init__()\n",
        "        self.gnn  = conv_layers\n",
        "        self.pool = pool_fn\n",
        "        self.lin  = torch.nn.Linear(hidden_dim, num_classes)\n",
        "    def forward(self, data):\n",
        "        # ——— 여기서 int → float 캐스팅 ———\n",
        "        x = data.x.float()\n",
        "        edge_index, batch = data.edge_index, data.batch\n",
        "        x = self.gnn(x, edge_index)\n",
        "        x = self.pool(x, batch)\n",
        "        return self.lin(x)\n",
        "\n",
        "# 3) 실험 설정\n",
        "hidden_dim  = 128\n",
        "num_classes = dataset.num_classes\n",
        "n_epochs    = 20\n",
        "conv_layers = GNNConvLayer(dataset.num_features, hidden_dim, num_layers=2)\n",
        "\n",
        "# (train_loader, valid_loader, test_loader, device, evaluate 함수는 이전에 정의된 그대로 사용)\n",
        "# ——— evaluate 함수 정의 ———\n",
        "def evaluate(model, loader):\n",
        "    model.eval()                             # 평가 모드로 전환\n",
        "    correct = 0\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)            # GPU로 올리고\n",
        "        batch.x = batch.x.float()           # int → float (GCNConv 에러 방지용)\n",
        "        out = model(batch)                  # 순전파\n",
        "        pred = out.argmax(dim=1)            # 예측 클래스\n",
        "        correct += int((pred == batch.y.view(-1)).sum())  # 맞춘 개수 누적\n",
        "    return correct / len(loader.dataset)    # 전체 대비 정확도\n",
        "\n",
        "\n",
        "# 4) 학습·평가 헬퍼\n",
        "def run_experiment(pool_fn, name):\n",
        "    model     = GraphPoolNet(conv_layers, hidden_dim, num_classes, pool_fn).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    best_valid = 0.0\n",
        "\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            batch = batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(batch)\n",
        "            loss = F.cross_entropy(out, batch.y.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        train_acc = evaluate(model, train_loader)\n",
        "        valid_acc = evaluate(model, valid_loader)\n",
        "        test_acc  = evaluate(model, test_loader)\n",
        "        best_valid = max(best_valid, valid_acc)\n",
        "\n",
        "    print(f\"{name} pooling → \"\n",
        "          f\"Train: {train_acc*100:.2f}%  \"\n",
        "          f\"Valid: {valid_acc*100:.2f}%  \"\n",
        "          f\"Test : {test_acc*100:.2f}%\")\n",
        "\n",
        "# 5) 세 가지 pooling 방식 비교\n",
        "run_experiment(global_mean_pool, 'mean')  # 평균\n",
        "run_experiment(global_add_pool,  'sum')   # 합\n",
        "run_experiment(global_max_pool,  'max')   # 최대\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwdWWW4XigAk",
        "outputId": "82b8bada-73a2-4d97-8d14-7b4c55312906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean pooling → Train: 96.23%  Valid: 97.96%  Test : 96.84%\n",
            "sum pooling → Train: 96.43%  Valid: 97.54%  Test : 96.99%\n",
            "max pooling → Train: 96.23%  Valid: 97.96%  Test : 96.69%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7JXsMTBgeOI"
      },
      "source": [
        "# Submission\n",
        "\n",
        "You will need to submit four files on Gradescope to complete this notebook.\n",
        "\n",
        "1.   Your completed *XCS224W_Colab2.ipynb*. From the \"File\" menu select \"Download .ipynb\" to save a local copy of your completed Colab.\n",
        "2.  *ogbn-arxiv_node.csv*\n",
        "3.  *ogbg-molhiv_graph_valid.csv*\n",
        "4.  *ogbg-molhiv_graph_test.csv*\n",
        "\n",
        "Download the csv files by selecting the *Folder* icon on the left panel.\n",
        "\n",
        "To submit your work, zip the files downloaded in steps 1-4 above and submit to gradescope. **NOTE:** DO NOT rename any of the downloaded files."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ZGKqVEbbMEzf",
        "rwKbzhHUAckZ",
        "u05Z14TRYPGn",
        "e7JXsMTBgeOI"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}